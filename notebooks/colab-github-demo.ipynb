{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aymenayachi/4G-3GPP-ETSI-machine-learning-anomaly-detection/blob/main/notebooks/colab-github-demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.manifold import TSNE\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Ensure GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "3kAzpMNnguuz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_uap(model, data_loader, max_iter=50, eps=0.1):\n",
        "    \"\"\"\n",
        "    Generate UAP using DeepFool-like method (as in paper).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    uap = torch.zeros(1, 3, 32, 32).to(device)  # Adjust input shape for your dataset\n",
        "    for _ in range(max_iter):\n",
        "        for x, _ in data_loader:\n",
        "            x = x.to(device)\n",
        "            x_perturbed = x + uap\n",
        "            x_perturbed = torch.clamp(x_perturbed, 0, 1)\n",
        "\n",
        "            # Compute gradient to maximize misclassification\n",
        "            x_perturbed.requires_grad = True\n",
        "            outputs = model(x_perturbed)\n",
        "            loss = -torch.sum(outputs)  # Maximize loss to fool the model\n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            grad = x_perturbed.grad.data\n",
        "            uap = uap + eps * torch.sign(grad.mean(dim=0, keepdim=True))\n",
        "            uap = torch.clamp(uap, -eps, eps)  # L-infinity constraint\n",
        "    return uap.detach()"
      ],
      "metadata": {
        "id": "M-i9d9QWgzeF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_fingerprints(model, uap, dataset, n_clusters=100):\n",
        "    \"\"\"\n",
        "    Generate fingerprints using K-means clustering on the victim model's last layer.\n",
        "    \"\"\"\n",
        "    # Extract features from the last layer\n",
        "    features = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, _ in dataset:\n",
        "            x = x.to(device)\n",
        "            feat = model.features(x)  # Adjust to extract features before classification head\n",
        "            features.append(feat.cpu().numpy())\n",
        "    features = np.concatenate(features, axis=0)\n",
        "\n",
        "    # Cluster features using K-means\n",
        "    kmeans = KMeans(n_clusters=n_clusters)\n",
        "    cluster_labels = kmeans.fit_predict(features)\n",
        "\n",
        "    # Select one sample per cluster\n",
        "    selected_indices = []\n",
        "    for i in range(n_clusters):\n",
        "        cluster_samples = np.where(cluster_labels == i)[0]\n",
        "        selected_indices.append(np.random.choice(cluster_samples))\n",
        "\n",
        "    # Generate fingerprints\n",
        "    fingerprints = []\n",
        "    for idx in selected_indices:\n",
        "        x, _ = dataset[idx]\n",
        "        x_perturbed = x + uap\n",
        "        with torch.no_grad():\n",
        "            logits_clean = model(x.unsqueeze(0).to(device))\n",
        "            logits_perturbed = model(x_perturbed.unsqueeze(0).to(device))\n",
        "        fingerprints.append(torch.cat([logits_clean, logits_perturbed], dim=1))\n",
        "\n",
        "    return torch.cat(fingerprints, dim=0)"
      ],
      "metadata": {
        "id": "BCv4ddqfg33V"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContrastiveEncoder(nn.Module):\n",
        "    def __init__(self, input_dim=200, hidden_dim=128, output_dim=64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def contrastive_loss(z1, z2, temperature=0.5):\n",
        "    \"\"\"\n",
        "    Supervised contrastive loss (Eq. 7 in the paper).\n",
        "    \"\"\"\n",
        "    z = torch.cat([z1, z2], dim=0)\n",
        "    similarity = torch.mm(z, z.T) / temperature\n",
        "    labels = torch.cat([torch.arange(z1.size(0)) for _ in range(2)], dim=0)\n",
        "    loss = nn.CrossEntropyLoss()(similarity, labels)\n",
        "    return loss\n",
        "\n",
        "# Training loop\n",
        "encoder = ContrastiveEncoder().to(device)\n",
        "optimizer = optim.Adam(encoder.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(100):\n",
        "    for batch in fingerprint_dataloader:\n",
        "        # Assume batch contains victim, piracy, and homologous fingerprints\n",
        "        victim_fp, piracy_fp, homo_fp = batch\n",
        "        z_victim = encoder(victim_fp)\n",
        "        z_piracy = encoder(piracy_fp)\n",
        "        z_homo = encoder(homo_fp)\n",
        "\n",
        "        # Positive pairs: victim-piracy, Negative pairs: victim-homo\n",
        "        loss = contrastive_loss(z_victim, z_piracy) + contrastive_loss(z_victim, z_homo, neg_weight=2.0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "IV5FMwfig9Zr",
        "outputId": "7d08af99-1a31-4e14-cb2d-dbd22ca2e702",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'fingerprint_dataloader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-6d7973244038>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfingerprint_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Assume batch contains victim, piracy, and homologous fingerprints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mvictim_fp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpiracy_fp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhomo_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fingerprint_dataloader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qVy2LCBkhBO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qk3Jg9e2Se2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oFH3hfgVSezZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rg5vw5IqSewN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kjVi2aYsSes8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YO5Oy27FSemk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ... [Previous imports and setup code] ...\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# ========== Device Setup ==========\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ========== Dataset Preparation ==========\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# ========== Model Definition ==========\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 8 * 8, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "fPWL40D6SefD",
        "outputId": "f11fe7b5-e8ac-4eef-b1fd-920f69537fa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Improved UAP Generation ==========\n",
        "def generate_uap(model, data_loader, max_iter=50, eps=0.1, overshoot=0.02):\n",
        "    model.eval()\n",
        "    uap = torch.zeros(1, 3, 32, 32).to(device)\n",
        "    for _ in range(max_iter):\n",
        "        for x, _ in data_loader:\n",
        "            x = x.to(device)\n",
        "            x_perturbed = torch.clamp(x + uap, 0, 1)\n",
        "            x_perturbed.requires_grad = True\n",
        "\n",
        "            outputs = model(x_perturbed)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            loss = torch.sum(outputs[:, preds])\n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            grad = x_perturbed.grad.data\n",
        "            perturbation = (grad.mean(dim=0, keepdim=True) + 1e-8) * eps\n",
        "            uap = torch.clamp(uap + perturbation, -eps, eps)\n",
        "\n",
        "    return uap.detach()"
      ],
      "metadata": {
        "id": "RihchTpFWSlS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Enhanced Fingerprint Generation ==========\n",
        "def generate_fingerprints(model, uap, dataset, n_clusters=100):\n",
        "    features = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, _ in DataLoader(dataset, batch_size=100):\n",
        "            x = x.to(device)\n",
        "            feat = model.features(x)\n",
        "            features.append(feat.cpu())\n",
        "    features = torch.cat(features).numpy()\n",
        "\n",
        "    kmeans = KMeans(n_clusters=n_clusters)\n",
        "    kmeans.fit(features)\n",
        "    cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "    selected_indices = [np.argmin(np.linalg.norm(features - center, axis=1)) for center in cluster_centers]\n",
        "\n",
        "    fingerprints = []\n",
        "    for idx in selected_indices:\n",
        "        x, _ = dataset[idx]\n",
        "        x = x.to(device)\n",
        "        x_perturbed = torch.clamp(x + uap, 0, 1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits_clean = model(x.unsqueeze(0))\n",
        "            logits_perturbed = model(x_perturbed.unsqueeze(0))\n",
        "\n",
        "        fingerprints.append(torch.cat([logits_clean, logits_perturbed], dim=1))\n",
        "\n",
        "    return torch.cat(fingerprints, dim=0)"
      ],
      "metadata": {
        "id": "wAaB5CR1WVqn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Model Training Functions ==========\n",
        "def train_model(model, train_loader, epochs=10):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}')\n",
        "    return model\n",
        "\n",
        "# ========== Contrastive Encoder ==========\n",
        "class ContrastiveEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ContrastiveEncoder, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(20, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "cJz0g4n6ShvH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Main Workflow ==========\n",
        "if __name__ == \"__main__\":\n",
        "    victim_model = SimpleCNN().to(device)\n",
        "    piracy_model = SimpleCNN().to(device)\n",
        "    homo_model = SimpleCNN().to(device)\n",
        "\n",
        "    print(\"Training victim model...\")\n",
        "    victim_model = train_model(victim_model, trainloader, epochs=10)\n",
        "\n",
        "    print(\"Training homologous model...\")\n",
        "    homo_model = train_model(homo_model, trainloader, epochs=10)\n",
        "\n",
        "    print(\"Generating UAP...\")\n",
        "    victim_uap = generate_uap(victim_model, trainloader)\n",
        "\n",
        "    print(\"Generating fingerprints...\")\n",
        "    victim_fps = generate_fingerprints(victim_model, victim_uap, trainset)\n",
        "    piracy_fps = generate_fingerprints(piracy_model, victim_uap, trainset)\n",
        "    homo_fps = generate_fingerprints(homo_model, victim_uap, trainset)\n",
        "\n",
        "    class FingerprintDataset(Dataset):\n",
        "        def __init__(self, victim_fps, piracy_fps, homo_fps):\n",
        "            self.positive_pairs = [(victim_fps[i], piracy_fps[i]) for i in range(len(victim_fps))]\n",
        "            self.negative_pairs = [(victim_fps[i], homo_fps[i]) for i in range(len(victim_fps))]\n",
        "            self.all_pairs = self.positive_pairs + self.negative_pairs\n",
        "            self.labels = [1]*len(self.positive_pairs) + [0]*len(self.negative_pairs)\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.all_pairs)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            return self.all_pairs[idx], self.labels[idx]\n",
        "\n",
        "    dataset = FingerprintDataset(victim_fps, piracy_fps, homo_fps)\n",
        "    fingerprint_dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "\n",
        "    encoder = ContrastiveEncoder().to(device)\n",
        "    optimizer = optim.Adam(encoder.parameters(), lr=1e-3)\n",
        "\n",
        "    for epoch in range(50):\n",
        "        total_loss = 0\n",
        "        for (anchor, compare), labels in fingerprint_dataloader:\n",
        "            anchor, compare, labels = anchor.to(device), compare.to(device), labels.to(device)\n",
        "\n",
        "            z_anchor = encoder(anchor)\n",
        "            z_compare = encoder(compare)\n",
        "\n",
        "            logits = torch.cosine_similarity(z_anchor, z_compare, dim=-1)\n",
        "            loss = nn.BCEWithLogitsLoss()(logits, labels.float())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Loss: {total_loss/len(fingerprint_dataloader):.4f}')\n",
        "\n",
        "    def verify_ownership(suspect_model, victim_uap, encoder, threshold=0.8):\n",
        "        suspect_fps = generate_fingerprints(suspect_model, victim_uap, testset)\n",
        "        with torch.no_grad():\n",
        "            z_victim = encoder(victim_fps.to(device))\n",
        "            z_suspect = encoder(suspect_fps.to(device))\n",
        "            similarities = torch.cosine_similarity(z_victim, z_suspect, dim=1)\n",
        "        return similarities.mean().item() > threshold\n",
        "\n",
        "    print(\"\\nVerification results:\")\n",
        "    print(f\"Piracy model: {verify_ownership(piracy_model, victim_uap, encoder)}\")\n",
        "    print(f\"Homologous model: {verify_ownership(homo_model, victim_uap, encoder)}\")\n"
      ],
      "metadata": {
        "id": "gautvb23Wdo-",
        "outputId": "4e77da33-0118-4295-bf6c-4ad3bd217d53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 808
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training victim model...\n",
            "Epoch 1, Loss: 2.0818\n",
            "Epoch 2, Loss: 1.6856\n",
            "Epoch 3, Loss: 1.4857\n",
            "Epoch 4, Loss: 1.3577\n",
            "Epoch 5, Loss: 1.2727\n",
            "Epoch 6, Loss: 1.1988\n",
            "Epoch 7, Loss: 1.1371\n",
            "Epoch 8, Loss: 1.0795\n",
            "Epoch 9, Loss: 1.0298\n",
            "Epoch 10, Loss: 0.9833\n",
            "Training homologous model...\n",
            "Epoch 1, Loss: 2.0603\n",
            "Epoch 2, Loss: 1.6724\n",
            "Epoch 3, Loss: 1.4670\n",
            "Epoch 4, Loss: 1.3443\n",
            "Epoch 5, Loss: 1.2617\n",
            "Epoch 6, Loss: 1.2005\n",
            "Epoch 7, Loss: 1.1418\n",
            "Epoch 8, Loss: 1.0878\n",
            "Epoch 9, Loss: 1.0391\n",
            "Epoch 10, Loss: 0.9934\n",
            "Generating UAP...\n",
            "Generating fingerprints...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found array with dim 4. KMeans expected <= 2.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9950c2f97a63>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generating fingerprints...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mvictim_fps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_fingerprints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvictim_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvictim_uap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mpiracy_fps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_fingerprints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpiracy_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvictim_uap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mhomo_fps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_fingerprints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhomo_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvictim_uap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-7ea70cd9ddcf>\u001b[0m in \u001b[0;36mgenerate_fingerprints\u001b[0;34m(model, uap, dataset, n_clusters)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mcluster_centers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1452\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m         \"\"\"\n\u001b[0;32m-> 1454\u001b[0;31m         X = validate_data(\n\u001b[0m\u001b[1;32m   1455\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2944\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2945\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2946\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1099\u001b[0m             )\n\u001b[1;32m   1100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1102\u001b[0m                 \u001b[0;34m\"Found array with dim %d. %s expected <= 2.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m                 \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with dim 4. KMeans expected <= 2."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rT2z9_SHWkud"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "colab-github-demo.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}